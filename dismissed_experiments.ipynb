{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'loader' from 'C:\\\\Users\\\\Roxanne El Baff\\\\Documents\\\\projects\\\\acl2020-news-editorial-analysis\\\\loader.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import loader\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import utility\n",
    "import spacy\n",
    "\n",
    "importlib.reload(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def clean_content_ending(article_content, id_):\n",
    "    clean_end_content = end_pars[end_pars['id'] == int(id_.replace(\".txt\", \"\"))]['content'].values[0]\n",
    "    idx_ = article_content.find(clean_end_content) +len(clean_end_content)\n",
    "    if  article_content.find(clean_end_content)>-1:\n",
    "        print('OK')\n",
    "    else:\n",
    "        print('ACHTUNG1 ',id_)\n",
    "    #print('removed: ', row['content'][idx_:])\n",
    "    removed= article_content[idx_:]\n",
    "    content = article_content[:idx_]\n",
    "    return content, removed\n",
    "\n",
    "end_pars = load.pars_features_df[load.pars_features_df['discourse_level'] == 'end'][['id', 'content']]\n",
    "corpus_txts_paths = glob.glob('corpus/*.txt')\n",
    "removed_txts = []\n",
    "for path in corpus_txts_paths:\n",
    "    new_path = path.replace('corpus', 'cleaned_corpus')\n",
    "    id_ = path.split('\\\\')[-1]\n",
    "\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "        text , removed= clean_content_ending(text, id_)\n",
    "        removed_txts.append(removed)\n",
    "        with open(new_path, 'w', encoding=\"utf-8\") as w:\n",
    "            w.write(text)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removed_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"corpus\" is set. It contains the 6000 annotation\n",
      "\"corpus\" is set with ideology_intensity: \"extreme\" and \"lean\"\n"
     ]
    }
   ],
   "source": [
    "load = loader.loader()\n",
    "corpus = load.corpus\n",
    "corpus = load.add_ideology_intensity()\n",
    "#articles_per_ideology_intensity = load.get_article_dfs_per_ideology(ideology='ideology_intensity', include_content=True)\n",
    "#articles_per_ideology_orientation =  load.get_article_dfs_per_ideology(ideology='political_pole', include_content=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Liberal and conservative df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "liberal_articles_df = articles_per_ideology_orientation['liberal']\n",
    "conservative_articles_df = articles_per_ideology_orientation['conservative']\n",
    "\n",
    "#liberal_articles_df=liberal_articles_df.apply(apply_add_majority, axis=1)\n",
    "#conservative_articles_df=conservative_articles_df.apply(apply_add_majority, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liberal_articles_df['majority'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conservative_articles_df['majority'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_stats(df):\n",
    "    for split, split_df in df.groupby('majority'):\n",
    "        print(split)\n",
    "        print(split_df['split_label'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_stats(conservative_articles_df)\n",
    "print_stats(liberal_articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ADUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discourse_adus_df = load.load_discourse_adus()\n",
    "liberal_articles_df=liberal_articles_df.join(discourse_adus_df )\n",
    "conservative_articles_df=conservative_articles_df.join(discourse_adus_df )\n",
    "conservative_articles_df = conservative_articles_df[['majority']]\n",
    "liberal_articles_df = liberal_articles_df[['ids', 'split_label',\n",
    "       'majority', 'content', 'adu_anecdote', 'adu_other',\n",
    "       'adu_statistics', 'adu_testimony']]\n",
    "liberal_articles_df.rename(columns={\"majority\":\"liberal_majority\"}, inplace=True)\n",
    "conservative_articles_df.rename(columns={\"majority\":\"conservative_majority\"}, inplace=True)\n",
    "df = liberal_articles_df.join(conservative_articles_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test(df):\n",
    "    return df[df['split_label'] == 'train'], df[df['split_label'] == 'test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features - lexicon based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import lexicons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = lexicons.count_nrc_emotions_and_sentiments(df, text_column= 'content')\n",
    "df = lexicons.count_mpqa_arg(df, text_column= 'content')\n",
    "df = lexicons.count_mpqa_subj_obj(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import pandas as pd\n",
    "\n",
    "def train_eval(train_df, eval_df):\n",
    "    # Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column is the text with type str, and the second column in the label with type int.\n",
    "    #train_df = pd.DataFrame(train_data)\n",
    "\n",
    "    #eval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0], ['Example eval senntence belonging to class 2', 2]]\n",
    "    #eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "    # Create a ClassificationModel\n",
    "    model = ClassificationModel('bert', 'bert-base-uncased', num_labels=3, use_cuda=False, args={'reprocess_input_data': True, 'overwrite_output_dir': True})\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "\n",
    "    model.train_model(train_df)\n",
    "\n",
    "    # Evaluate the model\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "    #predictions, raw_outputs = model.predict([\"Some arbitary sentence\"])\n",
    "    return model, result, model_outputs, wrong_predictions\n",
    "\n",
    "def eval(wrong_predictions, model_outputs):\n",
    "    len(wrong_predictions)#from collections import Counter\n",
    "    #predictions, raw_outputs = model.predict([\"Some arbitary sentence\"])\n",
    "    #model_outputs[0].argmax()\n",
    "\n",
    "    labels = [x.argmax() for x in model_outputs]\n",
    "    print(Counter(labels))\n",
    "lib_model, lib_result, lib_model_outputs, lib_wrong_predictions = train_eval(train_liberal_df[['content', 'majority_int']], test_liberal_df[['content', 'majority_int']])\n",
    "eval(lib_wrong_predictions, lib_model_outputs)\n",
    "cons_model, cons_result, cons_model_outputs, cons_wrong_predictions  = train_eval(train_cons_df[['content', 'majority_int']], test_cons_df\n",
    "           [['content', 'majority_int']])\n",
    "eval(cons_wrong_predictions, cons_model_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
